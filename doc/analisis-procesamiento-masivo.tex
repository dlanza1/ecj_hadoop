Vivimos en el momento m\'as \'algido en la generaci\'on de informaci\'on, nunca antes habían existido plataformas que generaran la cantidad de información que se genera hoy en d\'ia. El hecho de que del análisis de grandes cantidades de información se puedan extraer valiosos datos, como estrategias de negocio, hacen que numerosas empresas y organizaciones almacenen cantidades ingentes de información para poder sacarle el máximo partido.

La generaci\'on de informaci\'on se est\'a produciendo en \'ambitos muy dispares, estos pueden ser redes sociales que mantienen millones de usuarios, grandes empresas con muchos clientes, laboratorios de f\'isica con redes de millones sensores y muchos otros ejemplos que podríamos mencionar. Todos ellos queriendo extraer el máximo valor a la información que recaban.

\subsubsection{Computaci\'on distribuida}

Cuando hablamos de cantidades de información\'on del orden de terabytes o petabytes no podemos pensar en otra cosa que no sea computación distribuida. Para una sola m\'aquina, el tiempo que conllevar\'ia procesar esas cantidades de datos podr\'ian ser del orden de a\~nos.

Por tanto, la computaci\'on distribuida es la \'unica soluci\'on con el hardware que hoy en d\'ia manejamos, que permit\'a analizar y manejar esas cantidades de información. Esta soluci\'on implica el uso de m\'ultiples computadores conectados a una red común, cada uno de las cuales tiene su propio procesador, arquitectura, memoria, etc. con lo que pueden ser una composición de m\'aquinas con diferentes configuraciones (heterog\'enea). En contraposici\'on a este modelo encontramos la computaci\'on paralela en una sola m\'aquina, que consiste en utilizar m\'as de un hilo de procesamiento simult\'aneamente en un mismo procesador. Idealmente, el procesamiento paralelo permite que un programa se ejecute m\'as r\'apido, en la pr\'actica, suele ser dif\'icil dividir un programa de forma que CPUs separadas ejecuten diferentes porciones del programa sin ninguna interacci\'on que deteriore esa mejora de rendimiento.

\subsection{Modelo computacional: Map/Reduce}

En este \'ambito, surge la necesidad de dise\~nar herramientas que puedan no s\'olo mantener esta informaci\'on, si no también que tengan la capacidad de analizarla y extraer el valor que se desea de una forma distribuida y con un modelo sencillo. 

\textit{Google}, el buscador de internet m\'as utilizado en el mundo, ha hecho frente a este problema antes que nadie ya que desde hace a\~nos maneja cantidades de información realmente grandes, es por esto que sus investigaciones y experiencia son avanzadas. Varios a\~nos atrás hicieron p\'ublica \cite{paper-mapreduce} una solución para el an\'alisis de grandes cantidades de datos de forma distribuida y con un modelo que da soluci\'on a la complejidad de dividir el problema para poder paralelizarlo, a este modelo se le conoce como Map/Reduce. Esta publicación ha dado pie a que se implemente una herramienta que se conoce con el nombre de Hadoop \cite{hadoop}, la cual se describe con m\'as detalle en un cap\'itulo posterior \verapartado{desarrollo-hadoop}. La creaci\'on de esta herramienta y el hecho de que se hayan obtenidos buenos resultados de su utilizaci\'on, ha provocado que surjan otras muchas herramientas a su alrededor, las cuales ayudan a diferentes tareas como el volcado de información (Sqoop), bases de datos para consulta (Impala), gestion de flujos de datos (Flume) y otras muchas.

La propuesta de computación distribuida que se hace en esta publicación, hace uso de un modelo computacional anteriormente conocido en la programación funcional. Este modelo se basa en aplicar dos funciones básicas conocidas como Map (mapeo) y Reduce (reducci\'on). La función de mapeo consiste en aplicar una transformación o procedimiento a todos los datos, obteniendo así la entrada de la siguiente fase, reducción, la cual consiste en ''resumir", aplicando la misma función a diferentes partes de la salida de la fase de mapeo, obteniendo finalmente la salida deseada. Numerosas fases de mapeo y reducción pueden ser concatenadas en el orden que se quiera con el fin de producir el resultado esperado. Este modelo es el implementado en Hadoop pero con algunas peculiaridades \verapartado{desarrollo-hadoop}.
