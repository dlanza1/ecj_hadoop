La herramienta de c\'omputo evolutivo ECJ es conocida entre otras cosas por su flexibilidad a la hora de poder desarrollar problemas en este ámbito, es por esto que la implementación que se plantea sigue la misma idea con el objetivo de que gran parte de los problemas que pueden ser implementados con esta herramienta, puedan hacer uso de la característica que se desarrolla en este trabajo de una forma sencilla.
\label{desarrollo-implementacion}

El propósito del trabajo es distribuir el costo computacional de la fase de evaluación de un algoritmo de computación evolutiva, es por esto que lo que debemos desarrollar es un Evaluator \ver{ecj-classes} el cual lleve a cabo la evaluación de los individuos.

Para poder hacer esto y que pueda ser este evaluador establecido en cualquier problema que se plantee en la herramienta, se debe crear una clase que extienda de la clase abstracta ec.Evaluator. Al heredar de una clase abstracta debemos implementar los métodos marcados como abstractos, los de la clase ec.Evaluator se muestran a continuación.

\begin{lstlisting}[language=Java]
    /** Evalua el fitness de una poblacion entera. */
    public abstract void evaluatePopulation(final EvolutionState state);

    /** Debe retornar true si la ejecucion del algoritmo debe detenerse por algun motivo.
    Un ejemplo es cuando se encuentra al individuo ideal */
    public abstract boolean runComplete(final EvolutionState state);
\end{lstlisting}

En ambos métodos recibimos un objeto EvaluationState el cual contiene el estado de la evaluación, eso engloba también a la población o subpoblaciones que es en lo que nosotros estamos interesados ya que debemos obtener cada individuo para evaluarlo.

Abordaremos en primer lugar el método runComplete por su simple implementación, la cual se puede observar a continuación:

\begin{lstlisting}[language=Java]
	public boolean runComplete(final EvolutionState state) {
		for (int x = 0; x < state.population.subpops.length; x++)
			for (int y = 0; y < state.population.subpops[x].individuals.length; y++)
				if (state.population.subpops[x].individuals[y].fitness.isIdealFitness())
					return true;
		
		return false;
	}
\end{lstlisting}

La implementación realizada se encarga únicamente de recorrer todos los individuos de la población (lo que conlleva recorrer cada subpoblaci\'on) y comprobar si el fitness de cada uno de los individuos es ideal haciendo uso del método .isIdealFitness(). En este caso, si alguno de los individuos posee un fitness ideal, se retorna true y si al recorrer todos los individuos ninguno es ideal, retornamos false.

\subsection{Aclaraciones previas}

Antes de entrar en detalle de la implementación del método evaluatePopulation, debemos realizar algunas consideraciones. Para realizar la evaluación de los individuos, no es solo necesario las características que definen a cada individuo (genotipo), si no también aspectos como que función de evaluación utilizar o parámetros que determinan la forma de evaluar el individuo, estos y otros aspectos necesarios est\'an contenidos en el objeto que representa el estado de la evaluación (EvaluationState). Es por esto que en cada uno de los procesos que queramos evaluar los individuos, se debe tener acceso a este objeto, para tal propósito haremos uso de dos funciones clave que poseen ECJ y el sistema de ficheros de Hadoop (HDFS).

Para que todos los procesos tengan acceso al estado de la evaluación, haremos uso de la característica "puntos de restauración" que posee ECJ, esta característica permite serializar todo el estado del proceso evolutivo y enviarlo a través de la red o guardarlo en algún fichero, en principio esta característica se ide\'o con el fin de que se pueda detener el proceso y reanudarlo por donde iba cuando se desee pero en esta implementación se utilizar\'a para distribuir el estado del problema entre las diferentes m\'aquinas que llevar\'an a cabo la evaluación.

Por otro lado, utilizaremos una característica de HDFS denominada cache distribuida, esta característica permite almacenar ficheros de forma que se distribuyan por todos y cada uno de los nodos que conforman el cluster, lo cual es ideal en este caso ya que si almacenamos aquí el estado de la evolución, todos los procesos que se distribuyan podrán acceder a el.

Debemos tener en cuenta que el estado de la evaluación contiene todos los individuos de la población, lo cual no es necesario distribuir por dos motivos principales: en primer lugar porque cada nodo evaluar\'a solo una parte de la población con lo que no necesita contener toda la población, y en segundo lugar porque la población ser\'a la entrada del trabajo de Hadoop la cual estará contenida en el sistema de ficheros (HDFS). Sabiendo \'esto, se distribuirá el estado de la evaluación sin la población, esta ser\'a proporcionada a través de los ficheros de entrada al trabajo, los cuales Hadoop se encarg\'a automáticamente de dividir y distribuir.

\subsubsection{Dos modelos para dos situaciones}\label{dos-modelos-para-dos-situaciones}

La implementación que se describirá a continuación aborda el problema ejecutando un trabajo de Hadoop que evalúa la población completa. Este trabajo tiene como entrada la población de individuos, \'esta es dividida y distribuida para su evaluación y finalmente se recaban estos resultados para asignar  las aptitudes (fitness) a los individuos. Este modelo debe ser utilizado para problemas donde se utilizan amplias poblaciones o con individuos cuya evaluación es costosa, aunque si la evaluación es notablemente costosa y la evaluación de un solo individuo puede ser paralizada quizás debamos tener en cuenta el modelo que se explica a continuación.

Existe otro enfoque en el que podemos utilizar un trabajo de Hadoop para evaluar cada uno de los individuos. Este modelo debe ser considerado cuando la evaluación de cada individuo toma varios minutos ya que tan solo la creación de cada trabajo conlleva varios segundos (~5 segundos). La idea que persigue este modelo es lanzar de forma paralela tantos trabajos como individuos haya en la población, procesando cada uno de ellos la aptitud del individuo que ha evaluado, finalmente como en el modelo anterior, se recaban los resultados para asignarselos a los individuos. Este enfoque se ha utilizado para dar solución al problema de reconocimiento facial que se estudia en el capitulo siguiente \verapartado{problema-facerecognition}.

\subsection{Paso 1: Distribución del estado de la evaluación}

Una vez descrito el proceso necesario para la evaluación distribuida de los individuos, pasamos ahora a describir la implementación realizada del método evaluatePopulation, la cual establece varias etapas que describimos a continuación. Cada una de estas etapas se ven representadas en el diagrama de flujo de datos visto en la sección anterior \ver{fases-evaluacion-un-trabajo}.

Con el objetivo de simplificar el desarrollo se ha implementado un cliente que lleva a cabo todas las tareas relacionadas con Hadoop:

\begin{lstlisting}[language=Java]
	HadoopClient hadoopClient = new HadoopClient(
											hdfs_address,
											hdfs_port,
											jobtracker_address,
											jobtracker_port);
			
	hadoopClient.setWorkFolder(work_folder);
\end{lstlisting}

El cliente implementado se ve caracterizado por cinco aspectos, las direcciones y puertos de los procesos principales de Hadoop y el directorio sobre el que se trabajar\'a en el sistema de ficheros (HDFS).

El primer paso que se lleva a cabo para la evaluación, es la creación y distribución del punto de restauración el cual almacena el estado de la evolución, esto se realiza en el siguiente fragmento de código:

\begin{lstlisting}[language=Java]
	//Extrae poblacion del estado de la evolucion
	LinkedList<Individual[]> individuals_tmp = new LinkedList<Individual[]>();
	for (Subpopulation subp : state.population.subpops){
		individuals_tmp.add(subp.individuals);
		subp.individuals = null;
	}
	
	//Crear punto de restauracion
	Checkpoint.setCheckpoint(state);
	
	//Restaurar poblacion en el estado de la evaluacion
	int i = 0;
	for (Individual[] individuals : individuals_tmp) {
		state.population.subpops[i++].individuals = individuals;
	}
			
	//Distribuir punto de restauracion
	hadoopClient.addCacheFile(new File("" + state.checkpointPrefix + "." + state.generation + ".gz"), true, true);
\end{lstlisting}

Como se mencionaba anteriormente, la población es eliminada del punto de restauración, para ello la extraemos del estado de la evaluación, creamos el punto de restauración y posteriormente la introducimos de nuevo. Por \'ultimo se distribuye el punto de restauración haciendo uso del cliente de Hadoop que utiliza la cache distribuida de HDFS para tal fin.

\subsection{Paso 2: Creación de la entrada del trabajo}

La población de individuos a evaluar conformar\'a la entrada del trabajo de MapReduce, por lo que debemos escribir cada uno de los individuos en HDFS para que después el trabajo puede leerlos desde ahi. Para ello, desde el evaluador llamamos al método createInput del cliente de Hadoop implementado. Este método consiste en lo siguiente:

\begin{lstlisting}[language=Java]
	Path input_file = new Path(work_folder.concat("/input/population.seq"));
	Writer writer = getSequenceFileWriter(input_file, IndividualIndexWritable.class, IndividualWritable.class);

	Subpopulation[] subpops = state.population.subpops;
	int len = subpops.length;
	for (int pop = 0; pop < len; pop++) {
		for (int indiv = 0; indiv < subpops[pop].individuals.length; indiv++) {
			if (!subpops[pop].individuals[x].evaluated){
				writer.append(new IndividualIndexWritable(pop, indiv), 
						      new IndividualWritable(state, subpops[pop].individuals[indiv]));
			}
		}
	}

	writer.close();
\end{lstlisting}

Creamos un fichero secuencial en el directorio de trabajo con el nombre "population.sql", el cual contendrá la población entera. Debemos tener en cuenta que la entrada de un trabajo de MapRedice est\'a compuesta por tuplas de clave-valor por lo que ese ser\'a el contenido de este fichero, donde la clave est\'a compuesta por dos números que identifican de forma inequívoca a cada individuo, lo población (pop) y el n\'umero de individuo dentro de ella (indiv), y el valor ser\'a el propio individuo que estará compuesto entre otras cosas por el genotipo.

Una vez creado el fichero se recorren todas las subpoblaciones e individuos y se a\~naden al fichero que contiene la población. Se debe comprobar si el individuo no est\'a evaluado, ya que pudiera provenir de una generación anterior y eso supone que ya est\'a evaluado y no es necesario volver a evaluarlo.

\subsection{Paso 3: Creación y ejecución del trabajo de MapReduce (evaluación)}

Una vez distribuido el estado de la evolución y generada la entrada del problema, podemos definir el trabajo de MapReduce para lanzarlo en Hadoop. Como se ha comentado anteriormente \verapartado{fase-map-eval}, se usar\'a la fase de Map y no la de reduce para la evaluación de individuos, para ello debemos definir la función que se llevar\'a a cabo en esta fase. Se dede implementar un mapper que ser\'a una clase que extienda de la clase org.apache.hadoop.mapreduce.Mapper y debe implementar el método map. En nuestro caso, el propósito es la evaluación de los individuos por lo que la definimos de la siguiente manera:

\begin{lstlisting}[language=Java]
	@Override
	protected void map(IndividualIndexWritable key, IndividualWritable value, Context context)
			throws IOException, InterruptedException {

		Individual ind = value.getIndividual();
		
		//Evaluar individuo
		SimpleProblemForm problem = ((SimpleProblemForm) state.evaluator.p_problem);
		problem.evaluate(state, ind, key.getSubpopulation(), 0);

		//Escribir fitness
		context.write(key, new FitnessWritable(state, ind.fitness));
	}
\end{lstlisting}

Como podemos observar, la clave y el valor que recibimos son los mismos que contienen nuestro fichero de entrada (IndividualIndexWritable y IndividualWritable), este método map ser\'a llamado por Hadoop con cada uno de los registros de los ficheros de entrada, en nuestro caso por cada individuo. Lo que hacemos es, en primer lugar, obtener el individuo desde el valor recibido, acceder al problema que estará definido por el usuario (problem) desde el estado de la evaluación (state) y una vez adquirido el problema, evaluamos al individuo. El resultado de la evaluación (el fitness) compondrá la salida de la función map junto al identificador del individuo, que si recordamos es la clave de entrada a la función map.

Definida la función, estamos en condiciones de crear el trabajo de MapReduce e iniciarlo. Mostramos en primer lugar el procedimiento a llevar a cabo y a continuación lo explicamos.

\begin{lstlisting}[language=Java]
	//Creacion del trabajo
	Job job = new Job(conf);
	job.setJarByClass(EvaluationMapper.class);
	
	//Configurar entrada
	job.setInputFormatClass(SequenceFileInputFormat.class);
	Path input_directory = new Path(work_folder.concat("/input"));
	SequenceFileInputFormat.addInputPath(job, input_directory);
	
	//Configurar fase Map
	job.setMapperClass(EvaluationMapper.class);
	job.setMapOutputKeyClass(IndividualIndexWritable.class);
	job.setMapOutputValueClass(FitnessWritable.class);

	//Configurar fase Reduce
	job.setNumReduceTasks(0);

	//Configurar salida
	job.setOutputFormatClass(SequenceFileOutputFormat.class);
	job.setOutputKeyClass(IndividualIndexWritable.class);
	job.setOutputValueClass(FitnessWritable.class);
	Path output_directory = new Path(work_folder.concat("/output"));
	hdfs.delete(output_directory, true);
	SequenceFileOutputFormat.setOutputPath(job, output_directory);
\end{lstlisting}

En primer lugar creamos el objeto que representa el trabajo de MapReduce, posteriormente configuramos la entrada indicando que es un fichero secuencial y el directorio de entrada. Ahora debemos definir las fases del trabajo, en primer lugar la fase Map, indicando la clase que implementa el método mostrado anteriormente (map) y en segundo lugar la fase de reduce de la cual vamos a prescindir por lo que tan solo debemos indicar que no habra ninguna tarea reduce. Por \'ultimo, configuramos la salida indicando de que tipos est\'a compuesta, los cuales coinciden con los tipos de salida del Mapper implementado, borramos el directorio de salida que contendría el resultado de la evaluación de la generación anterior y indicamos el directorio de salida.

Una vez configurado lo inicioamos de la siguiente manera:

\begin{lstlisting}[language=Java]
	job.waitForCompletion(true);
\end{lstlisting}

Esto enviar'a el trabajo al cluster que hayamos indicado en el cliente de Hadoop y evaluar\'a todos los individuos contenidos en los ficheros de entrada.

\subsection{Paso 4: Asignaci\'on de resultados}

El \'ultimo paso es asignarle a los individuos los resultados obtenidos de forma distribuida con el trabajo en MapReduce para continuar así de forma normal el proceso de la evoluci\'on. El trabajo ha generado en el directorio de salida un conjunto de ficheros (uno por cada proceso Mapper) que contienen la salida de la función map la cual est\'a compuesta por tuplas del identificador del usuario y el fitness calculado.

El procedimiento para recabar los resultados se lleva a cabo en el cliente de Hadoop implementado llamando al metodo readFitness y su implementación es la siguiente:

\begin{lstlisting}[language=Java]
	Path output_directory = new Path(work_folder.concat("/output"));

	// Obtenemos todos los ficheros que produjo el trabajo
	FileStatus[] output_files = hdfs.listStatus(output_directory);

	// Establecemos los fitness calculados
	IndividualIndexWritable key;
	SequenceFile.Reader reader;
	for (FileStatus output_file : output_files) {
		reader = new SequenceFile.Reader(conf, SequenceFile.Reader.file(output_file.getPath()));

		key = new IndividualIndexWritable();
		while (reader.next(key)) {
			Individual individual = state.population.subpops[key.getSubpopulation()].individuals[key.getIndividual()];
			
			//Establecemos el fitness
			reader.getCurrentValue(new FitnessWritable(state, individual.fitness));
			
			//Marcamos individuo como evaluado
			individual.evaluated = true;
		}

		reader.close();
	}
\end{lstlisting}

En primer lugar, obtenemos una lista con todos los ficheros de salida producidos y a continuación recorremos cada uno de ellos. Leemos cada registro del fichero y asignamos al individuo correspondiente (obtenemos subpoblacion y numero de individuo desde la clave leída) el fitness extraído y finalmente marcamos cada individuo como evaluado.















